<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Self-Hosted Search</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}" />
  </head>
  <body data-smart-min-results="{{ smart_min_results }}">
    <main class="container">
      <h1>Self-Hosted Search</h1>
      <div class="layout">
        <section class="search-panel" aria-label="Search panel">
          <form id="search-form" role="search" aria-label="Search local index">
            <label class="sr-only" for="query-input">Search query</label>
            <input
              id="query-input"
              name="q"
              type="search"
              placeholder="Search your index..."
              value="{{ query|default('') }}"
              autocomplete="off"
              required
            />
            <button type="submit">Search</button>
          </form>
          <section id="status" aria-live="polite" aria-busy="false"></section>
          <section id="focused-status" class="focused-status" aria-live="polite" aria-busy="false" hidden>
            <h2>Focused crawl activity</h2>
            <p id="focused-message">Searching deeper…</p>
            <pre id="focused-log" aria-label="Focused crawl log tail"></pre>
          </section>
          <section id="results" aria-live="polite"></section>
        </section>
        <aside class="llm-panel" aria-label="LLM Assist">
          <h2>LLM Assist</h2>
          <p id="llm-status-text" class="llm-status">Checking Ollama...</p>
          <div class="field" id="llm-model-wrapper">
            <label for="llm-model">Model</label>
            <select id="llm-model" disabled>
              <option value="">Detecting models...</option>
            </select>
          </div>
          <label class="toggle">
            <input type="checkbox" id="llm-toggle" />
            <span>Use LLM for discovery (focused crawl)</span>
          </label>
          <div id="llm-instructions" class="llm-instructions" hidden>
            <p id="llm-instructions-text">Ollama is not ready. Install and run it locally to enable AI-assisted discovery:</p>
            <p><strong>macOS:</strong></p>
            <pre><code>brew install --cask ollama
ollama serve
ollama pull llama3.1:8b-instruct</code></pre>
            <p>
              See the
              <a href="https://ollama.com/" target="_blank" rel="noopener noreferrer">official Ollama documentation</a>
              for more details.
            </p>
          </div>
          <section class="cheat-sheet" aria-label="Fielded search tips">
            <h3>Fielded search tips</h3>
            <ul>
              <li><code>title:"python packaging"</code> — boost matches in page titles.</li>
              <li><code>h1h2:guide</code> — focus on headings and subheadings.</li>
              <li><code>lang:en packaging tutorial</code> — filter by detected language.</li>
            </ul>
          </section>
        </aside>
      </div>
    </main>
    <script>
      const form = document.getElementById("search-form");
      const queryInput = document.getElementById("query-input");
      const statusEl = document.getElementById("status");
      const resultsEl = document.getElementById("results");
      const focusedSection = document.getElementById("focused-status");
      const focusedMessage = document.getElementById("focused-message");
      const focusedLog = document.getElementById("focused-log");
      const llmStatusEl = document.getElementById("llm-status-text");
      const llmModelSelect = document.getElementById("llm-model");
      const llmToggle = document.getElementById("llm-toggle");
      const llmInstructions = document.getElementById("llm-instructions");
      const llmInstructionsText = document.getElementById("llm-instructions-text");
      const defaultInstructionsText = llmInstructionsText ? llmInstructionsText.textContent : "";

      const SMART_MIN_RESULTS = Number(document.body.dataset.smartMinResults || "1");
      const STORAGE_MODEL_KEY = "ollama_model";
      const STORAGE_TOGGLE_KEY = "smart_use_llm";

      const state = {
        pollTimer: null,
        indexTimer: null,
        baselineIndexTime: 0,
        latestIndexTime: 0,
        autoRefresh: false,
        lastQuery: queryInput.value.trim(),
        lastParams: "",
      };

      const savedToggle = localStorage.getItem(STORAGE_TOGGLE_KEY);
      if (savedToggle !== null) {
        llmToggle.checked = savedToggle === "true";
      }

      llmToggle.addEventListener("change", () => {
        localStorage.setItem(STORAGE_TOGGLE_KEY, String(llmToggle.checked));
      });

      llmModelSelect.addEventListener("change", () => {
        localStorage.setItem(STORAGE_MODEL_KEY, llmModelSelect.value);
      });

      function updateStatus(text, busy = false) {
        statusEl.textContent = text;
        statusEl.setAttribute("aria-busy", busy ? "true" : "false");
      }

      function renderResults(results) {
        if (!results.length) {
          resultsEl.innerHTML = '<p class="empty">No results yet. Trigger a crawl or wait for focused discovery.</p>';
          return;
        }
        const fragment = document.createDocumentFragment();
        results.forEach((result) => {
          const article = document.createElement("article");
          article.className = "result";

          const heading = document.createElement("h2");
          const link = document.createElement("a");
          link.href = result.url;
          link.textContent = result.title || result.url;
          link.target = "_blank";
          link.rel = "noopener noreferrer";
          heading.appendChild(link);
          article.appendChild(heading);

          const url = document.createElement("p");
          url.className = "url";
          url.textContent = result.url || "";
          article.appendChild(url);

          if (result.lang) {
            const lang = document.createElement("p");
            lang.className = "lang";
            lang.textContent = `Language: ${result.lang}`;
            article.appendChild(lang);
          }

          if (result.snippet) {
            const snippet = document.createElement("p");
            snippet.className = "snippet";
            snippet.innerHTML = result.snippet;
            article.appendChild(snippet);
          }

          const score = document.createElement("p");
          score.className = "score";
          score.textContent = `Score: ${Number(result.score || 0).toFixed(3)}`;
          article.appendChild(score);

          fragment.appendChild(article);
        });
        resultsEl.innerHTML = "";
        resultsEl.appendChild(fragment);
      }

      function renderFocusedLog(lines) {
        focusedLog.textContent = lines.length ? lines.join("\n") : "Waiting for crawler output...";
      }

      function stopPolling() {
        if (state.pollTimer) {
          clearInterval(state.pollTimer);
          state.pollTimer = null;
        }
        if (state.indexTimer) {
          clearInterval(state.indexTimer);
          state.indexTimer = null;
        }
        state.autoRefresh = false;
        focusedSection.hidden = true;
        focusedSection.setAttribute("aria-busy", "false");
        renderFocusedLog([]);
      }

      async function fetchFocusedStatus() {
        try {
          const response = await fetch("/api/focused/status");
          if (!response.ok) {
            throw new Error(`Status request failed: ${response.status}`);
          }
          const payload = await response.json();
          const logTail = Array.isArray(payload.log_tail) ? payload.log_tail.slice(-3) : [];
          renderFocusedLog(logTail);
          focusedMessage.textContent = payload.running
            ? "Focused crawl running. New results will appear automatically."
            : "Focused crawl finished. Refreshing results…";
        } catch (error) {
          console.error(error);
          renderFocusedLog(["Unable to read focused crawl log."]);
        }
      }

      async function checkIndexUpdate() {
        try {
          const response = await fetch("/api/focused/last_index_time");
          if (!response.ok) {
            throw new Error(`Index status failed: ${response.status}`);
          }
          const payload = await response.json();
          const value = Number(payload.last_index_time || 0);
          state.latestIndexTime = Math.max(state.latestIndexTime, value);
          if (state.autoRefresh && value > state.baselineIndexTime) {
            state.autoRefresh = false;
            await runSearch({ auto: true });
          }
        } catch (error) {
          console.error(error);
        }
      }

      function startPolling(baseline) {
        state.baselineIndexTime = baseline || 0;
        state.latestIndexTime = baseline || 0;
        state.autoRefresh = true;
        if (state.pollTimer) {
          clearInterval(state.pollTimer);
        }
        if (state.indexTimer) {
          clearInterval(state.indexTimer);
        }
        focusedSection.hidden = false;
        focusedSection.setAttribute("aria-busy", "true");
        renderFocusedLog([]);
        fetchFocusedStatus();
        state.pollTimer = setInterval(fetchFocusedStatus, 3000);
        state.indexTimer = setInterval(checkIndexUpdate, 4000);
      }

      function buildParams(auto) {
        if (auto && state.lastParams) {
          return new URLSearchParams(state.lastParams);
        }
        const params = new URLSearchParams();
        params.set("q", queryInput.value.trim());
        if (llmToggle.checked) {
          params.set("llm", "on");
        } else {
          params.set("llm", "off");
        }
        if (llmModelSelect.value) {
          params.set("model", llmModelSelect.value);
        }
        state.lastParams = params.toString();
        return params;
      }

      async function runSearch({ auto = false } = {}) {
        const query = auto ? state.lastQuery : queryInput.value.trim();
        if (!query) {
          updateStatus("Enter a query to search your local index.");
          renderResults([]);
          stopPolling();
          return;
        }
        const params = buildParams(auto);
        params.set("q", query);
        state.lastQuery = query;

        updateStatus(auto ? "Refreshing results…" : "Searching index…", true);
        try {
          const response = await fetch(`/api/search?${params.toString()}`);
          if (!response.ok) {
            throw new Error(`Search failed: ${response.status}`);
          }
          const payload = await response.json();
          const results = payload.results || [];
          renderResults(results);
          if (payload.status === "focused_crawl_running") {
            updateStatus("Focused crawl running. Waiting for new documents…", true);
            startPolling(Number(payload.last_index_time || 0));
          } else {
            stopPolling();
            const count = results.length;
            if (!count) {
              updateStatus("No results found. Try a different query or wait for new data.");
            } else {
              updateStatus(`Found ${count} result${count === 1 ? "" : "s"}.`);
            }
          }
        } catch (error) {
          console.error(error);
          updateStatus("Search failed. Check console logs for details.");
          stopPolling();
        }
      }

      async function fetchLLMStatus() {
        try {
          const response = await fetch("/api/llm/status");
          if (!response.ok) {
            throw new Error(`Status request failed: ${response.status}`);
          }
          const status = await response.json();
          applyLLMStatus(status);
        } catch (error) {
          console.error(error);
          llmStatusEl.textContent = "Ollama: Unable to determine status";
          llmInstructions.hidden = false;
          llmModelSelect.disabled = true;
          llmToggle.disabled = true;
        }
      }

      async function fetchLLMModels() {
        try {
          const response = await fetch("/api/llm/models");
          if (!response.ok) {
            throw new Error(`Model request failed: ${response.status}`);
          }
          const payload = await response.json();
          populateModelSelect(payload.models || []);
        } catch (error) {
          console.error(error);
          populateModelSelect([]);
        }
      }

      function populateModelSelect(models) {
        llmModelSelect.innerHTML = "";
        const storedModel = localStorage.getItem(STORAGE_MODEL_KEY) || "";
        if (!models.length) {
          const option = document.createElement("option");
          option.value = "";
          option.textContent = "No local models detected";
          llmModelSelect.appendChild(option);
          llmModelSelect.disabled = true;
          if (llmInstructionsText) {
            llmInstructionsText.textContent =
              'Ollama is running but no models were found. Pull one (e.g. "ollama pull llama3.1:8b-instruct") to enable discovery:';
          }
          llmInstructions.hidden = false;
          return;
        }
        if (llmInstructionsText) {
          llmInstructionsText.textContent = defaultInstructionsText;
        }
        llmInstructions.hidden = true;
        const placeholder = document.createElement("option");
        placeholder.value = "";
        placeholder.textContent = "Use Ollama default";
        llmModelSelect.appendChild(placeholder);

        let matched = false;
        models.forEach((model) => {
          if (!model || !model.name) {
            return;
          }
          const option = document.createElement("option");
          option.value = model.name;
          option.textContent = model.name;
          if (model.name === storedModel) {
            option.selected = true;
            matched = true;
          }
          llmModelSelect.appendChild(option);
        });
        llmModelSelect.disabled = false;
        if (!matched && storedModel) {
          localStorage.removeItem(STORAGE_MODEL_KEY);
        }
      }

      function applyLLMStatus(status) {
        if (!status) {
          llmStatusEl.textContent = "Ollama: status unavailable";
          llmInstructions.hidden = false;
          llmModelSelect.disabled = true;
          llmToggle.disabled = true;
          return;
        }
        const { installed, running, host } = status;
        if (!installed) {
          llmStatusEl.textContent = "Ollama not installed";
          llmInstructions.hidden = false;
          llmModelSelect.disabled = true;
          llmToggle.disabled = true;
          return;
        }
        if (!running) {
          llmStatusEl.textContent = `Ollama installed on ${host}, but not running`;
          llmInstructions.hidden = false;
          llmModelSelect.disabled = true;
          llmToggle.disabled = true;
          return;
        }
        llmStatusEl.textContent = `Ollama ready at ${host}`;
        llmInstructions.hidden = true;
        llmModelSelect.disabled = false;
        llmToggle.disabled = false;
      }

      form.addEventListener("submit", (event) => {
        event.preventDefault();
        runSearch();
      });

      fetchLLMStatus();
      fetchLLMModels();

      if (queryInput.value.trim()) {
        runSearch();
      }
    </script>
  </body>
</html>
